# Professional Folder Structure Documentation

## ğŸ¯ Overview

This document describes the professional, production-ready folder structure for the Khmer Sentiment Analysis project. The structure follows industry best practices for ML/AI projects.

## ğŸ“‚ Complete Directory Tree

```
PROJECT/
â”‚
â”œâ”€â”€ data/                           # ğŸ“Š Data Directory
â”‚   â”œâ”€â”€ Data Collection - Sheet1.csv    # Raw collected data
â”‚   â””â”€â”€ data_cleaned_all.csv            # Preprocessed dataset
â”‚
â”œâ”€â”€ src/                            # ğŸ’» Source Code (Main Package)
â”‚   â”œâ”€â”€ __init__.py                     # Package initializer
â”‚   â”œâ”€â”€ config.py                       # Configuration & hyperparameters
â”‚   â”œâ”€â”€ preprocessing.py                # Text preprocessing module
â”‚   â”œâ”€â”€ data_loader.py                  # Data loading & preparation
â”‚   â”œâ”€â”€ feature_extraction.py          # Feature engineering
â”‚   â”œâ”€â”€ models.py                       # Traditional ML models
â”‚   â”œâ”€â”€ deep_learning.py               # Deep learning models (LSTM)
â”‚   â”œâ”€â”€ evaluation.py                   # Model evaluation & metrics
â”‚   â””â”€â”€ model_persistence.py            # Model saving/loading
â”‚
â”œâ”€â”€ models/                         # ğŸ¤– Model Artifacts
â”‚   â””â”€â”€ saved_models/                   # Trained model storage
â”‚       â”œâ”€â”€ best_model_*.pkl                # Serialized ML models
â”‚       â”œâ”€â”€ best_model_*.keras              # Keras/TensorFlow models
â”‚       â”œâ”€â”€ tokenizer_*.pkl                 # Text tokenizers
â”‚       â”œâ”€â”€ label_encoder_*.pkl             # Label encoders
â”‚       â””â”€â”€ best_model_metadata_*.json      # Model metadata
â”‚
â”œâ”€â”€ results/                        # ğŸ“ˆ Output & Analysis
â”‚   â”œâ”€â”€ figures/                        # Plots & visualizations
â”‚   â”‚   â”œâ”€â”€ confusion_matrix_*.png          # Confusion matrices
â”‚   â”‚   â”œâ”€â”€ model_comparison_*.png          # Comparison charts
â”‚   â”‚   â””â”€â”€ lstm_history_*.png              # Training curves
â”‚   â””â”€â”€ reports/                        # Performance reports
â”‚       â””â”€â”€ model_comparison_*.csv          # Metrics CSV
â”‚
â”œâ”€â”€ notebooks/                      # ğŸ““ Jupyter Notebooks
â”‚   â”œâ”€â”€ Model.ipynb                     # Main analysis notebook
â”‚   â””â”€â”€ Notebook.ipynb                  # Exploratory notebook
â”‚
â”œâ”€â”€ tests/                          # ğŸ§ª Unit Tests
â”‚   â”œâ”€â”€ __init__.py                     # Test package init
â”‚   â”œâ”€â”€ test_preprocessing.py           # Preprocessing tests
â”‚   â”œâ”€â”€ test_data_loader.py             # Data loader tests
â”‚   â”œâ”€â”€ test_models.py                  # Model tests
â”‚   â””â”€â”€ test_evaluation.py              # Evaluation tests
â”‚
â”œâ”€â”€ docs/                           # ğŸ“š Documentation
â”‚   â”œâ”€â”€ API_REFERENCE.md                # API documentation
â”‚   â”œâ”€â”€ USER_GUIDE.md                   # User guide
â”‚   â””â”€â”€ DEVELOPMENT.md                  # Development guide
â”‚
â”œâ”€â”€ scripts/                        # ğŸ”§ Utility Scripts (Optional)
â”‚   â”œâ”€â”€ download_data.py                # Data download script
â”‚   â”œâ”€â”€ benchmark.py                    # Benchmarking script
â”‚   â””â”€â”€ export_model.py                 # Model export utilities
â”‚
â”œâ”€â”€ train.py                        # ğŸš€ Main Training Script
â”œâ”€â”€ predict.py                      # ğŸ”® Prediction Script
â”œâ”€â”€ setup.py                        # ğŸ“¦ Package Setup File
â”œâ”€â”€ requirements.txt                # ğŸ“‹ Python Dependencies
â”œâ”€â”€ README.md                       # ğŸ“– Project Overview (Original)
â”œâ”€â”€ README_PROFESSIONAL.md          # ğŸ“– Professional README
â”œâ”€â”€ PROJECT_STRUCTURE.md            # ğŸ“ This File
â”œâ”€â”€ .gitignore                      # ğŸš« Git Ignore Rules
â””â”€â”€ LICENSE                         # âš–ï¸ License File

```

## ğŸ—‚ï¸ Directory Descriptions

### ğŸ“Š `/data/`
**Purpose**: Store all datasets (raw and processed)

**Contents**:
- Raw collected data from various sources
- Cleaned and preprocessed datasets
- Train/test splits (if saved separately)
- Data validation reports

**Best Practices**:
- Keep raw data immutable
- Version control for data splits
- Document data sources and preprocessing steps

### ğŸ’» `/src/`
**Purpose**: Main source code package

**Modules**:

1. **`config.py`**: Centralized configuration
   - File paths
   - Hyperparameters
   - Model settings
   - Constants

2. **`preprocessing.py`**: Text preprocessing
   - Khmer Unicode normalization
   - Slang handling
   - Text cleaning functions

3. **`data_loader.py`**: Data operations
   - Load CSV/JSON data
   - Data cleaning
   - Train-test splitting

4. **`feature_extraction.py`**: Feature engineering
   - TF-IDF vectorization
   - Class weight computation
   - Feature transformation

5. **`models.py`**: Traditional ML models
   - Pipeline creation
   - Hyperparameter grids
   - Model training functions

6. **`deep_learning.py`**: Deep learning models
   - LSTM/BiLSTM architectures
   - Sequence preparation
   - Model training

7. **`evaluation.py`**: Model evaluation
   - Metrics calculation
   - Model comparison
   - Visualization functions

8. **`model_persistence.py`**: Persistence layer
   - Model saving/loading
   - Metadata management
   - Report generation

### ğŸ¤– `/models/saved_models/`
**Purpose**: Store trained model artifacts

**Contents**:
- Serialized models (.pkl, .keras)
- Tokenizers and encoders
- Model metadata (JSON)
- Version timestamps

**Naming Convention**:
```
best_model_{model_name}_{timestamp}.{extension}
best_model_metadata_{timestamp}.json
tokenizer_{type}_{timestamp}.pkl
```

### ğŸ“ˆ `/results/`
**Purpose**: Store analysis outputs and reports

**Subdirectories**:
- `figures/`: Plots and visualizations
- `reports/`: CSV reports and metrics

**Best Practices**:
- Timestamp all outputs
- Organize by experiment/run
- Keep figures in publication-ready format

### ğŸ““ `/notebooks/`
**Purpose**: Exploratory analysis and experimentation

**Usage**:
- Interactive data exploration
- Prototyping new features
- Visualization experiments
- Should import from `src/` modules

### ğŸ§ª `/tests/`
**Purpose**: Unit tests for code quality

**Structure**:
- Mirror `src/` structure
- Each module has corresponding test file
- Use pytest framework

**Run Tests**:
```bash
pytest tests/ -v
pytest tests/ --cov=src
```

### ğŸ“š `/docs/`
**Purpose**: Project documentation

**Contents**:
- API reference
- User guides
- Development guidelines
- Architecture documentation

## ğŸš€ Usage Workflows

### Workflow 1: Training Pipeline
```bash
# Step 1: Configure settings
vim src/config.py

# Step 2: Run training
python train.py

# Step 3: Check results
ls results/reports/
```

### Workflow 2: Making Predictions
```bash
# Single prediction
python predict.py --model_path models/saved_models/best_model_*.pkl \
                  --text "á¢ááŸ’áá”á‘ááŸ’á˜áŸ‚áš"

# Batch prediction
python predict.py --model_path models/saved_models/best_model_*.pkl \
                  --input_file data/new_data.csv \
                  --output_file results/predictions.csv
```

### Workflow 3: Development
```bash
# Step 1: Create new feature branch
git checkout -b feature/new-model

# Step 2: Add new model to src/models.py
vim src/models.py

# Step 3: Write tests
vim tests/test_models.py

# Step 4: Run tests
pytest tests/

# Step 5: Train with new model
python train.py
```

## ğŸ“‹ File Naming Conventions

### Code Files
- Use snake_case: `feature_extraction.py`
- Descriptive names: `model_persistence.py` not `utils.py`
- Test files: `test_{module_name}.py`

### Data Files
- Raw data: `data_raw_{source}_{date}.csv`
- Cleaned data: `data_cleaned_{version}.csv`
- Versioning: Use dates or semantic versions

### Model Files
- Include timestamp: `model_{type}_{timestamp}.pkl`
- Metadata: `metadata_{timestamp}.json`
- Descriptive names: `best_model_logistic_regression_20241226.pkl`

### Result Files
- Timestamp required: `comparison_20241226_143022.csv`
- Descriptive prefix: `confusion_matrix_lstm_*.png`

## ğŸ”§ Configuration Management

All configurations in `src/config.py`:

```python
# Paths
DATA_DIR = 'data/'
MODELS_DIR = 'models/saved_models/'
RESULTS_DIR = 'results/'

# Hyperparameters
TFIDF_MAX_FEATURES = 5000
LSTM_UNITS = 64
CV_FOLDS = 3

# Model Settings
SCORING_METRIC = 'f1_macro'
RANDOM_STATE = 42
```

## ğŸ¯ Benefits of This Structure

1. **Modularity**: Each component has single responsibility
2. **Reusability**: Functions can be imported and reused
3. **Testability**: Easy to write and run unit tests
4. **Scalability**: Easy to add new models/features
5. **Collaboration**: Clear structure for team development
6. **Reproducibility**: Version control and configuration management
7. **Production-Ready**: Can be deployed as package
8. **Documentation**: Self-documenting code structure

## ğŸ“¦ Package Installation

Install as editable package for development:

```bash
pip install -e .
```

Install with extras:

```bash
# With deep learning dependencies
pip install -e .[deep_learning]

# With development tools
pip install -e .[dev]
```

## ğŸ”„ Version Control

### What to commit:
- All code in `src/`
- Scripts (`train.py`, `predict.py`)
- Tests
- Documentation
- Configuration files
- `requirements.txt`
- `setup.py`

### What NOT to commit:
- Data files (use `.gitignore`)
- Model artifacts (too large)
- Results/outputs
- `__pycache__/`
- `.ipynb_checkpoints/`
- IDE configuration

### Sample `.gitignore`:
```
# Data
data/*
!data/.gitkeep

# Models
models/saved_models/*
!models/saved_models/.gitkeep

# Results
results/figures/*
results/reports/*
!results/**/.gitkeep

# Python
__pycache__/
*.pyc
*.pyo
*.egg-info/

# Jupyter
.ipynb_checkpoints/

# IDE
.vscode/
.idea/
```

## ğŸ“ Learning Resources

- **Cookiecutter Data Science**: Industry-standard project structure
- **Scikit-learn**: Best practices for ML pipelines
- **TensorFlow**: Model organization guidelines
- **Python Packaging**: Creating installable packages

---

**Maintained by**: [Your Name]  
**Last Updated**: December 26, 2025  
**Version**: 1.0
